{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper functions\n",
    "\n",
    "def get_feature_vector(tweet):\n",
    "    \"\"\"\n",
    "    Transform a given tweet to feature vector representaion\n",
    "    \"\"\"\n",
    "    words = tweet.split()\n",
    "    feature_vector = []\n",
    "    for i in range(len(words) - 1):\n",
    "        word = words[i]\n",
    "        if vocab.get(word) is not None:\n",
    "            feature_vector.append(vocab.get(word))\n",
    "    if len(words) >= 1:\n",
    "        if vocab.get(words[-1]) is not None:\n",
    "            feature_vector.append(vocab.get(words[-1]))\n",
    "    return feature_vector\n",
    "\n",
    "def process_tweets(csv_file):\n",
    "    \"\"\"\n",
    "    Invoke get_feature_vector to batch process these tweets,\n",
    "    and return the processed tweets and labels\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    print('Generating feature vectors...')\n",
    "    with open(csv_file, 'r') as csv:\n",
    "        lines = csv.readlines()\n",
    "        total = len(lines)\n",
    "        for i in tqdm(range(len(lines))):\n",
    "            line = lines[i]\n",
    "            tweet_id, label, tweet = line.split(',')\n",
    "            feature_vector = get_feature_vector(tweet)\n",
    "            tweets.append(feature_vector)\n",
    "            if int(label) == 4:\n",
    "                label = 1\n",
    "            labels.append(int(label))\n",
    "    return tweets, np.array(labels)\n",
    "\n",
    "def top_n_words(pkl_file_name, N, shift=0):\n",
    "    \"\"\"\n",
    "    Return the top-N words from the unigrams file\n",
    "    \"\"\"\n",
    "    with open(pkl_file_name, 'rb') as pkl_file:\n",
    "        freq_dist = pickle.load(pkl_file)\n",
    "    most_common = freq_dist.most_common(N)\n",
    "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
    "    return words\n",
    "\n",
    "def get_glove_vectors(vocab):\n",
    "    \"\"\"\n",
    "    Return the glove vectors by combining the top-N words\n",
    "    with the Glove file\n",
    "    \"\"\"\n",
    "    print('\\nLooking for GLOVE vectors...')\n",
    "    glove_vectors = {}\n",
    "    found = 0\n",
    "    with open(glove_twitter_vectors, 'r', encoding=\"utf8\") as glove_file:\n",
    "        for line in glove_file:\n",
    "            tokens = line.split()\n",
    "            word = tokens[0]\n",
    "            if vocab.get(word):\n",
    "                vector = [float(e) for e in tokens[1:]]\n",
    "                glove_vectors[word] = np.array(vector)\n",
    "                found += 1\n",
    "    print('Found %d words in GLOVE' % found)\n",
    "    return glove_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1048956 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1048956/1048956 [00:06<00:00, 162544.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for GLOVE vectors...\n",
      "Found 70388 words in GLOVE\n",
      "\n",
      "==> Training set: (839297, 60)  Validation set: (52689, 60)  Testing set: (156970, 60) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training & testing data, as well as some parameters.\n",
    "\n",
    "# Assign some paths\n",
    "processed_csv = '/home/mingj/project/tweet_training_data/large/sentiment140-processed.csv'\n",
    "unigrams_file = '/home/mingj/project/tweet_training_data/large/sentiment140-freqdist.pkl'\n",
    "bigrams_file = '/home/mingj/project/tweet_training_data/large/sentiment140-freqdist-bi.pkl'\n",
    "glove_twitter_vectors = '/home/mingj/project/tweet_training_data/glove.twitter.27B.200d.txt'\n",
    "\n",
    "# Define some hyperparameters\n",
    "vocab_size = 90000\n",
    "max_length = 60 # 40->60\n",
    "batch_size = 128\n",
    "epochs = 20 # 10->20\n",
    "learning_rate = 0.0001 # 0.001->0.0001\n",
    "filters = 600\n",
    "kernel_size = 3\n",
    "dim = 200\n",
    "\n",
    "# Prepare integerized tweets and labels\n",
    "vocab = top_n_words(unigrams_file, vocab_size, shift=1)\n",
    "tweets, labels = process_tweets(processed_csv)\n",
    "tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
    "shuffled_indices = np.random.permutation(tweets.shape[0])\n",
    "tweets = tweets[shuffled_indices]\n",
    "labels = labels[shuffled_indices]\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n",
    "glove_vectors = get_glove_vectors(vocab)\n",
    "for word, i in vocab.items():\n",
    "    glove_vector = glove_vectors.get(word)\n",
    "    if glove_vector is not None:\n",
    "        embedding_matrix[i] = glove_vector\n",
    "\n",
    "# Split the data to training, validation, and testing sets\n",
    "rnd_indices = np.random.rand(len(labels)) < 0.8\n",
    "train_x = tweets[rnd_indices]\n",
    "train_y = labels[rnd_indices]\n",
    "remain_x = tweets[~rnd_indices]\n",
    "remain_y = labels[~rnd_indices]\n",
    "rnd_indices2 = np.random.rand(len(remain_y)) < 0.25\n",
    "val_x = remain_x[rnd_indices2]\n",
    "val_y = remain_y[rnd_indices2]\n",
    "test_x = remain_x[~rnd_indices2]\n",
    "test_y = remain_y[~rnd_indices2]\n",
    "print('\\n==> Training set:', train_x.shape, ' Validation set:', val_x.shape, ' Testing set:', test_x.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 60, 200)           18000200  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 60, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 18,176,969\n",
      "Trainable params: 18,176,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the network\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dense(1))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr = learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_grads=False, write_images=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingj/anaconda3/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 839297 samples, validate on 52689 samples\n",
      "Epoch 1/20\n",
      "839297/839297 [==============================] - 418s 499us/step - loss: 0.3915 - acc: 0.8327 - val_loss: 0.3506 - val_acc: 0.8511\n",
      "Epoch 2/20\n",
      "839297/839297 [==============================] - 417s 497us/step - loss: 0.3530 - acc: 0.8511 - val_loss: 0.3324 - val_acc: 0.8594\n",
      "Epoch 3/20\n",
      "839297/839297 [==============================] - 417s 497us/step - loss: 0.3395 - acc: 0.8574 - val_loss: 0.3257 - val_acc: 0.8643\n",
      "Epoch 4/20\n",
      "839297/839297 [==============================] - 416s 496us/step - loss: 0.3302 - acc: 0.8618 - val_loss: 0.3248 - val_acc: 0.8650\n",
      "Epoch 5/20\n",
      "839297/839297 [==============================] - 415s 495us/step - loss: 0.3223 - acc: 0.8658 - val_loss: 0.3166 - val_acc: 0.8687\n",
      "Epoch 6/20\n",
      "839297/839297 [==============================] - 415s 495us/step - loss: 0.3159 - acc: 0.8685 - val_loss: 0.3130 - val_acc: 0.8701\n",
      "Epoch 7/20\n",
      "839297/839297 [==============================] - 415s 495us/step - loss: 0.3102 - acc: 0.8708 - val_loss: 0.3116 - val_acc: 0.8715\n",
      "Epoch 8/20\n",
      "839297/839297 [==============================] - 416s 496us/step - loss: 0.3050 - acc: 0.8732 - val_loss: 0.3167 - val_acc: 0.8726\n",
      "Epoch 9/20\n",
      "839297/839297 [==============================] - 416s 495us/step - loss: 0.3003 - acc: 0.8754 - val_loss: 0.3093 - val_acc: 0.8724\n",
      "Epoch 10/20\n",
      "839297/839297 [==============================] - 416s 496us/step - loss: 0.2964 - acc: 0.8775 - val_loss: 0.3060 - val_acc: 0.8700\n",
      "Epoch 11/20\n",
      "839297/839297 [==============================] - 422s 503us/step - loss: 0.2927 - acc: 0.8788 - val_loss: 0.3163 - val_acc: 0.8715\n",
      "Epoch 12/20\n",
      "839297/839297 [==============================] - 422s 503us/step - loss: 0.2889 - acc: 0.8806 - val_loss: 0.3072 - val_acc: 0.8741\n",
      "Epoch 13/20\n",
      "839297/839297 [==============================] - 423s 504us/step - loss: 0.2852 - acc: 0.8821 - val_loss: 0.3097 - val_acc: 0.8738\n",
      "Epoch 14/20\n",
      "839297/839297 [==============================] - 425s 507us/step - loss: 0.2821 - acc: 0.8837 - val_loss: 0.3115 - val_acc: 0.8746\n",
      "Epoch 15/20\n",
      "839297/839297 [==============================] - 428s 510us/step - loss: 0.2790 - acc: 0.8852 - val_loss: 0.3037 - val_acc: 0.8743\n",
      "Epoch 16/20\n",
      "839297/839297 [==============================] - 416s 496us/step - loss: 0.2757 - acc: 0.8869 - val_loss: 0.3012 - val_acc: 0.8759\n",
      "Epoch 17/20\n",
      "839297/839297 [==============================] - 418s 498us/step - loss: 0.2728 - acc: 0.8878 - val_loss: 0.3087 - val_acc: 0.8749\n",
      "Epoch 18/20\n",
      "839297/839297 [==============================] - 430s 513us/step - loss: 0.2702 - acc: 0.8892 - val_loss: 0.3012 - val_acc: 0.8756\n",
      "Epoch 19/20\n",
      "839297/839297 [==============================] - 419s 499us/step - loss: 0.2670 - acc: 0.8907 - val_loss: 0.3144 - val_acc: 0.8754\n",
      "Epoch 20/20\n",
      "839297/839297 [==============================] - 420s 500us/step - loss: 0.2640 - acc: 0.8917 - val_loss: 0.3117 - val_acc: 0.8735\n"
     ]
    }
   ],
   "source": [
    "# Training the network and give the model.\n",
    "\n",
    "model.fit(train_x, train_y, batch_size = batch_size, epochs = epochs, verbose = 1, callbacks = [tensorboard], validation_data = (val_x, val_y))\n",
    "model.save('sentiment_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "156970/156970 [==============================] - 83s 532us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3103561493066714, 0.8721475441167102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance on testing set\n",
    "\n",
    "print(model.metrics_names)\n",
    "model.evaluate(test_x, test_y, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
