{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper functions\n",
    "\n",
    "def analyze_tweet(tweet):\n",
    "    result = {}\n",
    "    result['POS_EMOS'] = tweet.count('EMO_POS')\n",
    "    result['NEG_EMOS'] = tweet.count('EMO_NEG')\n",
    "    words = tweet.split()\n",
    "    result['WORDS'] = len(words)\n",
    "    bigrams = get_bigrams(words)\n",
    "    result['BIGRAMS'] = len(bigrams)\n",
    "    return result, words, bigrams\n",
    "\n",
    "def get_bigrams(tweet_words):\n",
    "    bigrams = []\n",
    "    num_words = len(tweet_words)\n",
    "    for i in range(num_words - 1):\n",
    "        bigrams.append((tweet_words[i], tweet_words[i + 1]))\n",
    "    return bigrams\n",
    "\n",
    "def get_bigram_freqdist(bigrams):\n",
    "    freq_dict = {}\n",
    "    for bigram in bigrams:\n",
    "        if freq_dict.get(bigram):\n",
    "            freq_dict[bigram] += 1\n",
    "        else:\n",
    "            freq_dict[bigram] = 1\n",
    "    counter = Counter(freq_dict)\n",
    "    return counter\n",
    "\n",
    "def top_n_words(pkl_file_name, N, shift=0):\n",
    "    with open(pkl_file_name, 'rb') as pkl_file:\n",
    "        freq_dist = pickle.load(pkl_file)\n",
    "    most_common = freq_dist.most_common(N)\n",
    "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
    "    return words\n",
    "\n",
    "def top_n_bigrams(pkl_file_name, N, shift=0):\n",
    "    with open(pkl_file_name, 'rb') as pkl_file:\n",
    "        freq_dist = pickle.load(pkl_file)\n",
    "    most_common = freq_dist.most_common(N)\n",
    "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize some variables\n",
    "\n",
    "N = 20\n",
    "processed_csv = 'D:\\\\COMP90024_Assignment2\\\\sentiment analysis\\\\data_set_3\\\\sentiment140-processed.csv'\n",
    "unique_words_file = 'D:\\\\COMP90024_Assignment2\\\\sentiment analysis\\\\data_set_3\\\\sentiment140-uniquewords.txt'\n",
    "unigrams_file = 'D:\\\\COMP90024_Assignment2\\\\sentiment analysis\\\\data_set_3\\\\sentiment140-freqdist.pkl'\n",
    "bigrams_file = 'D:\\\\COMP90024_Assignment2\\\\sentiment analysis\\\\data_set_3\\\\sentiment140-freqdist-bi.pkl'\n",
    "\n",
    "num_tweets, num_pos_tweets, num_neg_tweets, num_neu_tweets = 0, 0, 0, 0\n",
    "num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
    "num_words, num_unique_words, min_words, max_words = 0, 0, 1e6, 0\n",
    "num_bigrams, num_unique_bigrams = 0, 0\n",
    "all_words = []\n",
    "all_bigrams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1048956/1048956 [00:07<00:00, 143216.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# read from csv\n",
    "\n",
    "with open(processed_csv, 'r') as csv:\n",
    "    lines = csv.readlines()\n",
    "    num_tweets = len(lines)\n",
    "    for i in tqdm(range(len(lines))):\n",
    "        line = lines[i]\n",
    "        t_id, label, tweet = line.strip().split(',')\n",
    "        label = int(label)\n",
    "        if label == 4:\n",
    "            num_pos_tweets += 1\n",
    "        elif label == 0:\n",
    "            num_neg_tweets += 1\n",
    "        else:\n",
    "            num_neu_tweets += 1\n",
    "        result, words, bigrams = analyze_tweet(tweet)\n",
    "        num_pos_emojis += result['POS_EMOS']\n",
    "        num_neg_emojis += result['NEG_EMOS']\n",
    "        max_emojis = max(\n",
    "            max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
    "        num_words += result['WORDS']\n",
    "        min_words = min(min_words, result['WORDS'])\n",
    "        max_words = max(max_words, result['WORDS'])\n",
    "        all_words.extend(words)\n",
    "        num_bigrams += result['BIGRAMS']\n",
    "        all_bigrams.extend(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating frequency distribution...\n",
      "Saved uni-frequency distribution to D:\\COMP90024_Assignment2\\sentiment analysis\\data_set_3\\sentiment140-freqdist.pkl\n",
      "Saved bi-frequency distribution to D:\\COMP90024_Assignment2\\sentiment analysis\\data_set_3\\sentiment140-freqdist-bi.pkl\n",
      "Calculating top-N results...\n"
     ]
    }
   ],
   "source": [
    "# write results to files\n",
    "\n",
    "num_emojis = num_pos_emojis + num_neg_emojis\n",
    "unique_words = list(set(all_words))\n",
    "with open(unique_words_file, 'w') as uwf:\n",
    "    uwf.write('\\n'.join(unique_words))\n",
    "num_unique_words = len(unique_words)\n",
    "num_unique_bigrams = len(set(all_bigrams))\n",
    "print('\\nCalculating frequency distribution...')\n",
    "\n",
    "# Unigrams\n",
    "freq_dist = FreqDist(all_words)\n",
    "with open(unigrams_file, 'wb') as pkl_file:\n",
    "    pickle.dump(freq_dist, pkl_file)\n",
    "print('Saved uni-frequency distribution to %s' % unigrams_file)\n",
    "\n",
    "# Bigrams\n",
    "bigram_freq_dist = get_bigram_freqdist(all_bigrams)\n",
    "with open(bigrams_file, 'wb') as pkl_file:\n",
    "    pickle.dump(bigram_freq_dist, pkl_file)\n",
    "print('Saved bi-frequency distribution to %s' % bigrams_file)\n",
    "\n",
    "# Top-N results\n",
    "print('Calculating top-N results...')\n",
    "top_words = top_n_words(unigrams_file, N)\n",
    "top_bigrams = top_n_bigrams(bigrams_file, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Analysis Statistics]\n",
      "Tweets => Total: 1048956, Positive: 248958, Negative: 799998, Neutral: 0\n",
      "Emojis => Total: 8287, Positive: 6649, Negative: 1638, Avg: 0.0079, Max: 16\n",
      "Words => Total: 13204097, Unique: 208566, Avg: 12.5878, Max: 40, Min: 0\n",
      "Bigrams => Total: 12157734, Unique: 2366158, Avg: 11.5903\n",
      "\n",
      "Top - 20 words ==> {'i': 0, 'to': 1, 'the': 2, 'a': 3, 'my': 4, 'and': 5, 'is': 6, 'it': 7, 'you': 8, 'in': 9, 'for': 10, 'im': 11, 'of': 12, 'me': 13, 'on': 14, 'so': 15, 'have': 16, 'but': 17, 'that': 18, 'not': 19}\n",
      "\n",
      "Top - 20 bigrams ==> {('i', 'have'): 0, ('in', 'the'): 1, ('going', 'to'): 2, ('i', 'dont'): 3, ('i', 'am'): 4, ('i', 'cant'): 5, ('to', 'go'): 6, ('have', 'to'): 7, ('i', 'was'): 8, ('to', 'be'): 9, ('but', 'i'): 10, ('and', 'i'): 11, ('for', 'the'): 12, ('to', 'the'): 13, ('on', 'the'): 14, ('i', 'miss'): 15, ('want', 'to'): 16, ('have', 'a'): 17, ('i', 'think'): 18, ('to', 'get'): 19}\n"
     ]
    }
   ],
   "source": [
    "# print analytical results\n",
    "\n",
    "print('\\n[Analysis Statistics]')\n",
    "print('Tweets => Total: %d, Positive: %d, Negative: %d, Neutral: %d' % (num_tweets, num_pos_tweets, num_neg_tweets, num_neu_tweets))\n",
    "print('Emojis => Total: %d, Positive: %d, Negative: %d, Avg: %.4f, Max: %d' % (num_emojis, num_pos_emojis, num_neg_emojis, num_emojis / float(num_tweets), max_emojis))\n",
    "print('Words => Total: %d, Unique: %d, Avg: %.4f, Max: %d, Min: %d' % (num_words, num_unique_words, num_words / float(num_tweets), max_words, min_words))\n",
    "print('Bigrams => Total: %d, Unique: %d, Avg: %.4f' % (num_bigrams, num_unique_bigrams, num_bigrams / float(num_tweets)))\n",
    "print('\\nTop -', N, 'words ==>', top_words)\n",
    "print('\\nTop -', N, 'bigrams ==>', top_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
